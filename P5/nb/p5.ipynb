{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e147cf4-cbc7-4a24-abd2-76b532bef5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -ls hdfs://nn:9000/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c50146e-85db-4c2a-b08a-edad9af98c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -D dfs.replication=1 -mkdir -p hdfs://nn:9000/data\n",
    "!hdfs dfs -D dfs.replication=1 -cp -f file:///nb/data/*.jsonl hdfs://nn:9000/\n",
    "!hdfs dfs -D dfs.replication=1 -cp -f file:///nb/data/*.csv hdfs://nn:9000/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3e90b10-5dbd-4b71-ab1a-31ee857a7d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import time\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import col, when, lit\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79b8df9d-2edc-4765-bf83-ece66968063b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/29 19:48:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = (SparkSession.builder.appName(\"cs544\")\n",
    "         .master(\"spark://boss:7077\")\n",
    "         .config(\"spark.executor.memory\", \"1G\")\n",
    "         .config(\"spark.sql.warehouse.dir\", \"hdfs://nn:9000/user/hive/warehouse\")\n",
    "         .enableHiveSupport()\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "535e8670-ab5e-4202-86f8-62d150334114",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "problems_df = spark.read.json(\"hdfs://nn:9000/problems.jsonl\")\n",
    "solutions_df = spark.read.json(\"hdfs://nn:9000/solutions.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "255cfb25-a1ae-4e80-81c8-6fb33537e75f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------+---------+---------+---------------+----------+---------------+-------------------------+------------------+--------------------+-------------+----------+------------+------+----------+\n",
      "|cf_contest_id|cf_index|cf_points|cf_rating|        cf_tags|difficulty|generated_tests|is_description_translated|memory_limit_bytes|                name|private_tests|problem_id|public_tests|source|time_limit|\n",
      "+-------------+--------+---------+---------+---------------+----------+---------------+-------------------------+------------------+--------------------+-------------+----------+------------+------+----------+\n",
      "|          322|       A|    500.0|     1000|            [0]|         7|             93|                    false|         256000000|322_A. Ciel and D...|           45|         1|           2|     2|         1|\n",
      "|          760|       D|   1000.0|     1600|         [1, 2]|        10|             51|                    false|         256000000|  760_D. Travel Card|            4|         2|           2|     2|         2|\n",
      "|          569|       E|   1500.0|     2600|         [3, 0]|        11|             99|                    false|         256000000| 569_E. New Language|           17|         3|           3|     2|         2|\n",
      "|          447|       B|   1000.0|     1000|         [0, 4]|         8|            100|                    false|         256000000|447_B. DZY Loves ...|           13|         4|           1|     2|         1|\n",
      "|         1292|       B|    750.0|     1700|[5, 6, 7, 0, 4]|         8|             91|                    false|         256000000|1292_B. Aroma's S...|          131|         5|           3|     2|         1|\n",
      "+-------------+--------+---------+---------+---------------+----------+---------------+-------------------------+------------------+--------------------+-------------+----------+------------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "problems_df.limit(5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6fcb7cd5-d144-4550-88fa-256edada57a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "217"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q1 RDD API\n",
    "q1_count = problems_df.rdd.filter(\n",
    "    lambda x: x.cf_rating >= 1600 and \n",
    "    x.private_tests > 0 and \n",
    "    \"_A.\" in x.name\n",
    ").count()\n",
    "q1_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b24fd126-7379-4f5c-ab42-edfce617bc98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "217"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q2: DataFrame API\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "q2_count = problems_df.filter(\n",
    "    expr(\"cf_rating >= 1600 AND private_tests > 0 AND name LIKE '%\\\\\\\\_A.%'\")\n",
    ").count()\n",
    "\n",
    "q2_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1f292c62-c5a7-4ada-a935-64e039ca8cc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "217"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q3: Spark SQL\n",
    "problems_df.write.mode(\"overwrite\").saveAsTable(\"problems\")\n",
    "q3_result = spark.sql(\"\"\"\n",
    "    SELECT COUNT(*) \n",
    "    FROM problems \n",
    "    WHERE cf_rating >= 1600 \n",
    "    AND private_tests > 0 \n",
    "    AND name LIKE '%\\\\\\\\_A.%'\n",
    "\"\"\").first()[0]\n",
    "q3_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1c1d1a14-29ea-449b-8ac3-2eab4b2ba8cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 35:==================================================>       (7 + 1) / 8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[language#686], functions=[count(1)])\n",
      "   +- HashAggregate(keys=[language#686], functions=[partial_count(1)])\n",
      "      +- FileScan parquet spark_catalog.default.solutions[language#686] Batched: true, Bucketed: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://nn:9000/user/hive/warehouse/solutions], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<language:string>, SelectedBucketsCount: 4 out of 4\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "#q4: Bucketed Solutions Table\n",
    "solutions_df.write.mode(\"overwrite\").bucketBy(4, \"language\").saveAsTable(\"solutions\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT language, COUNT(*)\n",
    "    FROM solutions\n",
    "    GROUP BY language\n",
    "\"\"\").explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9f054d17-f62c-4de1-ba62-0fb59a3694e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'problems': False,\n",
       " 'solutions': False,\n",
       " 'languages': True,\n",
       " 'problem_tests': True,\n",
       " 'sources': True,\n",
       " 'tags': True}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q5: Views in Warehouse\n",
    "csv_files = [\"languages\", \"problem_tests\", \"sources\", \"tags\"]\n",
    "for file in csv_files:\n",
    "    df = spark.read.csv(f\"hdfs://nn:9000/{file}.csv\", header=True)\n",
    "    df.createOrReplaceTempView(file)\n",
    "{\n",
    "    'problems': False,  \n",
    "    'solutions': False,\n",
    "    'languages': True,\n",
    "    'problem_tests': True, \n",
    "    'sources': True,\n",
    "    'tags': True \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "255844bf-ad32-42f2-89ff-0add51b448c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10576"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q6: Correct PYTHON3 Solutions from CODEFORCES\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT COUNT(*) as count\n",
    "    FROM solutions s\n",
    "    JOIN problems p ON s.problem_id = p.problem_id\n",
    "    JOIN sources src ON p.source = src.source\n",
    "    WHERE s.is_correct = True \n",
    "    AND s.language = 'PYTHON3'\n",
    "    AND src.source_name = 'CODEFORCES'\n",
    "\"\"\").first()[0]\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "02ff0d63-b7e9-49e8-b5bb-a19d12421fcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Easy': 409, 'Medium': 5768, 'Hard': 2396}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q7: Problem Difficulty Categories\n",
    "result = problems_df.select(\n",
    "    when(col(\"difficulty\") <= 5, \"Easy\")\n",
    "    .when(col(\"difficulty\") <= 10, \"Medium\")\n",
    "    .otherwise(\"Hard\")\n",
    "    .alias(\"difficulty_category\")\n",
    ").groupBy(\"difficulty_category\").count()\n",
    "difficulty_counts = {row.difficulty_category: row[\"count\"] for row in result.collect()}\n",
    "difficulty_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b315b8c3-5da2-4d77-987d-1c4032020406",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7604794502258301, 1.2660470008850098, 0.19753289222717285]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q8: Caching Performance\n",
    "filtered_tests = spark.sql(\"SELECT * FROM problem_tests WHERE is_generated = 'False'\")\n",
    "\n",
    "times = []\n",
    "start = time.time()\n",
    "filtered_tests.agg({\"input_chars\": \"avg\", \"output_chars\": \"avg\"}).collect()\n",
    "times.append(time.time() - start)\n",
    "filtered_tests.cache()\n",
    "start = time.time()\n",
    "filtered_tests.agg({\"input_chars\": \"avg\", \"output_chars\": \"avg\"}).collect()\n",
    "times.append(time.time() - start)\n",
    "start = time.time()\n",
    "filtered_tests.agg({\"input_chars\": \"avg\", \"output_chars\": \"avg\"}).collect()\n",
    "times.append(time.time() - start)\n",
    "filtered_tests.unpersist()\n",
    "\n",
    "times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "78fee005-e0d9-4495-9986-2fcb3701b094",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5929835263198762"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q9: Decision Tree Prediction\n",
    "codeforces_df = spark.sql(\"\"\"\n",
    "    SELECT p.* \n",
    "    FROM problems p\n",
    "    JOIN sources s ON p.source = s.source\n",
    "    WHERE s.source_name = 'CODEFORCES'\n",
    "\"\"\")\n",
    "train_df = codeforces_df.filter(\n",
    "    (col(\"cf_rating\") > 0) & \n",
    "    (col(\"problem_id\") % 2 == 0)\n",
    ")\n",
    "test_df = codeforces_df.filter(\n",
    "    (col(\"cf_rating\") > 0) & \n",
    "    (col(\"problem_id\") % 2 == 1)\n",
    ")\n",
    "missing_df = codeforces_df.filter(col(\"cf_rating\") == 0)\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"difficulty\", \"time_limit\", \"memory_limit_bytes\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "dt = DecisionTreeRegressor(\n",
    "    labelCol=\"cf_rating\",\n",
    "    featuresCol=\"features\",\n",
    "    maxDepth=5\n",
    ")\n",
    "pipeline = Pipeline(stages=[assembler, dt])\n",
    "model = pipeline.fit(train_df)\n",
    "predictions = model.transform(test_df)\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"cf_rating\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"r2\"\n",
    ")\n",
    "r2_score = evaluator.evaluate(predictions)\n",
    "r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "06c188ec-654c-4306-be8c-ee57c73c165b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1887.9377431906614, 1893.1106471816283, 1950.4728638818783)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q10: Missing cf_rating Predictions\n",
    "train_avg = train_df.agg({\"cf_rating\": \"avg\"}).collect()[0][0]\n",
    "test_avg = test_df.agg({\"cf_rating\": \"avg\"}).collect()[0][0]\n",
    "missing_predictions = model.transform(missing_df)\n",
    "missing_avg = missing_predictions.agg({\"prediction\": \"avg\"}).collect()[0][0]\n",
    "(train_avg, test_avg, missing_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc2ef5c-8363-4712-9b0c-6c8f6aa49627",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
